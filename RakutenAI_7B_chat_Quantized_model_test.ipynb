{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONOkvEEc5nu42ljerqsg+k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koji/GoogleColab/blob/main/RakutenAI_7B_chat_Quantized_model_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf3k33j4fAdb",
        "outputId": "a9008a62-53ea-43d1-a925-53743f6ce3d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-31 02:14:29--  https://huggingface.co/baxin/RakutenAI-7B-chat-Quantized-model/resolve/main/ggml-model-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.118, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.huggingface.co/repos/54/31/54317be958478aff580b31a0558c484fd5800b4252fef38e4ae819f82196f384/7f3ac926e7859262853624dba1c93484fdf0b6c82d6ac9def8a6f26ff282fd1a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-Q4_K_M.gguf%3B+filename%3D%22ggml-model-Q4_K_M.gguf%22%3B&Expires=1712110470&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMjExMDQ3MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU0LzMxLzU0MzE3YmU5NTg0NzhhZmY1ODBiMzFhMDU1OGM0ODRmZDU4MDBiNDI1MmZlZjM4ZTRhZTgxOWY4MjE5NmYzODQvN2YzYWM5MjZlNzg1OTI2Mjg1MzYyNGRiYTFjOTM0ODRmZGYwYjZjODJkNmFjOWRlZjhhNmYyNmZmMjgyZmQxYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LFoL5QqMmdXAxYmtqaV4Qv6cwbzvfN9HFNRim5%7E0cmBPKwRkQU0bn2nFk-ZlusY69QOTRB5vGgT39s6Sq1V9BFvma1S-t1nW-geEwVhK53Bu83JO8n5r84tblAm7BnnTeQlTXjk0rSCiQASXs8NEwN7TgGkttZAZVntZHUrJYbXX03GqhGJJuj5I42yuVQCgoYMrU4KeaVsnBxLOwz3mUr67L6-R4%7Ej-NR7iSFaLhH0Mtp7I5Vq1wJ4GZZrijFiHWPQgIwpnsM-CqLgRtXFoAjXOFGGdKieTWxoahmEgY2lK03atA8kZq4Db77xHgDZEI%7EztuT08b3pHXSS6%7Eq3erw__&Key-Pair-Id=KCD77M1F0VK2B [following]\n",
            "--2024-03-31 02:14:30--  https://cdn-lfs-us-1.huggingface.co/repos/54/31/54317be958478aff580b31a0558c484fd5800b4252fef38e4ae819f82196f384/7f3ac926e7859262853624dba1c93484fdf0b6c82d6ac9def8a6f26ff282fd1a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27ggml-model-Q4_K_M.gguf%3B+filename%3D%22ggml-model-Q4_K_M.gguf%22%3B&Expires=1712110470&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMjExMDQ3MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzU0LzMxLzU0MzE3YmU5NTg0NzhhZmY1ODBiMzFhMDU1OGM0ODRmZDU4MDBiNDI1MmZlZjM4ZTRhZTgxOWY4MjE5NmYzODQvN2YzYWM5MjZlNzg1OTI2Mjg1MzYyNGRiYTFjOTM0ODRmZGYwYjZjODJkNmFjOWRlZjhhNmYyNmZmMjgyZmQxYT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=LFoL5QqMmdXAxYmtqaV4Qv6cwbzvfN9HFNRim5%7E0cmBPKwRkQU0bn2nFk-ZlusY69QOTRB5vGgT39s6Sq1V9BFvma1S-t1nW-geEwVhK53Bu83JO8n5r84tblAm7BnnTeQlTXjk0rSCiQASXs8NEwN7TgGkttZAZVntZHUrJYbXX03GqhGJJuj5I42yuVQCgoYMrU4KeaVsnBxLOwz3mUr67L6-R4%7Ej-NR7iSFaLhH0Mtp7I5Vq1wJ4GZZrijFiHWPQgIwpnsM-CqLgRtXFoAjXOFGGdKieTWxoahmEgY2lK03atA8kZq4Db77xHgDZEI%7EztuT08b3pHXSS6%7Eq3erw__&Key-Pair-Id=KCD77M1F0VK2B\n",
            "Resolving cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)... 18.154.206.76, 18.154.206.88, 18.154.206.94, ...\n",
            "Connecting to cdn-lfs-us-1.huggingface.co (cdn-lfs-us-1.huggingface.co)|18.154.206.76|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4459436800 (4.2G) [binary/octet-stream]\n",
            "Saving to: ‘ggml-model-Q4_K_M.gguf’\n",
            "\n",
            "ggml-model-Q4_K_M.g 100%[===================>]   4.15G  63.3MB/s    in 65s     \n",
            "\n",
            "2024-03-31 02:15:36 (65.0 MB/s) - ‘ggml-model-Q4_K_M.gguf’ saved [4459436800/4459436800]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://huggingface.co/baxin/RakutenAI-7B-chat-Quantized-model/resolve/main/ggml-model-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuxqzmL7gdDU",
        "outputId": "abda4050-5b56-4054-dad1-afab497ba775"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.57.tar.gz (36.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.57-cp310-cp310-manylinux_2_35_x86_64.whl size=26393919 sha256=88c42762089ad444620d9db383bdefd50158335b9db6c1e7241e6c0c630f1b5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/7e/c0/00/e98d6e198f941c623da37b3f674354cbdccfcfb2cb9cf1133d\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import ctypes\n",
        "\n",
        "llm = Llama(model_path=\"/content/ggml-model-Q4_K_M.gguf\", n_gpu_layers=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVurU1tgoz7",
        "outputId": "41693fa1-9b87-4df3-871f-9486baece351"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from /content/ggml-model-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = models\n",
            "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 48000\n",
            "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
            "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,48000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,48000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,48000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/48000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 48000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.37 B\n",
            "llm_load_print_meta: model size       = 4.15 GiB (4.84 BPW) \n",
            "llm_load_print_meta: general.name     = models\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 2 '</s>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 32/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4251.79 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  3992.50 MiB\n",
            "..............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: n_batch    = 512\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =    64.00 MiB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =    93.75 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   255.56 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     9.00 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1060\n",
            "llama_new_context_with_model: graph splits = 4\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'llama.vocab_size': '48000', 'general.file_type': '15', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"東京について教えてください。\"\n",
        "input_prompt = f\"\"\"\n",
        "[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "p1XoCTaNhFN7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "    input_prompt,\n",
        "    max_tokens=2048,\n",
        ")"
      ],
      "metadata": {
        "id": "bX1kw52zj0v6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00670df9-a5a5-4372-d110-f372fdeb7483"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     445.64 ms\n",
            "llama_print_timings:      sample time =     317.63 ms /   370 runs   (    0.86 ms per token,  1164.89 tokens per second)\n",
            "llama_print_timings: prompt eval time =     445.14 ms /   142 tokens (    3.13 ms per token,   319.00 tokens per second)\n",
            "llama_print_timings:        eval time =   15464.08 ms /   369 runs   (   41.91 ms per token,    23.86 tokens per second)\n",
            "llama_print_timings:       total time =   17914.06 ms /   511 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[\"choices\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "N1PXTWAcj_8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d7c629-8086-428b-f7e5-37bce71daa22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<</SYS>>\n",
            "はい、東京は、日本の首都です。日本最大の都市で、世界でも有数の大都市です。東京は世界有数の経済都市であり、日本経済の中心地でもあります。東京には、世界有数の観光スポット、博物館、美術館、ショッピングセンター、レストラン、劇場、娯楽施設などが数多くあります。東京は、日本の文化、歴史、伝統、芸術、科学技術の最先端を体験するのに最適な場所です。東京では、東京スカイツリー、上野公園の梅花苑、東京スカイツリー、東京国立博物館、東京タワー、六本木ヒルズ、東京スカイツリータウンなどの観光スポットが人気です。東京を訪れるなら、江戸時代の伝統文化を体験することができる浅草寺や、日本料理や和菓子の食べ物を楽しめるおすすめの場所で、東京の伝統的な文化を体験することができます。東京には、歴史的に重要で、世界的に有名な場所もあります。たとえば、東京国立博物館、江戸東京博物館、東京国立博物館、東京国立博物館、東京国立博物館などの博物館があります。東京では、東京オリンピックや、世界最大級の国際会議やイベントなど、世界的に有名なイベントが開催されることもあります。東京には、世界有数の企業や、日本の政府機関や国際機関が数多くあります。東京は世界有数のビジネス都市であり、国際的な金融センターでもあります。東京で人気の観光スポットとしては、東京スカイツリー、上野公園の梅花苑、東京スカイツリー、東京タワー、六本木ヒルズ、東京スカイツリータウンなどの施設があります。東京を訪れるなら、江戸時代の伝統文化を体験することができる浅草寺や、日本料理や和菓子の食べ物を楽しめる\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yMjcTyIYkFHN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}